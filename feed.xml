<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://taptipalit.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://taptipalit.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-20T01:52:07+00:00</updated><id>https://taptipalit.github.io/feed.xml</id><title type="html">blank</title><subtitle>CRA CIFellow postdoctoral researcher at Purdue University. </subtitle><entry><title type="html">Pointer Analysis and Undefined Behavior in C programs</title><link href="https://taptipalit.github.io/blog/2020/pointer-analysis-and-undefined-behavior-in-c-programs/" rel="alternate" type="text/html" title="Pointer Analysis and Undefined Behavior in C programs"/><published>2020-11-24T00:00:00+00:00</published><updated>2020-11-24T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/pointer-analysis-and-undefined-behavior-in-c-programs</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/pointer-analysis-and-undefined-behavior-in-c-programs/"><![CDATA[<p>Recently, I came across the question — can Pointer Analysis algorithms such as Andersen’s algorithm and Steensgaard’s algorithm, could correctly detect undefined behavior (such as buffer overflow) in C programs? If yes, how? And if no, why not? And if it can’t how does it affect soundness of pointer analysis? This is a very interesting question and I’ll try to share my thoughts in this post.</p> <p>I believe the key to answering this question is to not classify behavior of C programs as defined or undefined when thinking about pointer analysis, but to think of pointer analysis as answering the question – which objects can a pointer point to during execution of the program as intended to by the programmer?</p> <p>Also, pointer analysis operates on an abstract model, it operates with limited knowledge of the full program. Moreover, some “knowledge” isn’t available at all at static analysis time at all (e.g. lengths of buffers read over the network, etc)</p> <p>In light of this, let’s take a look again at two interesting undefined behavior in C programs that affects pointers.</p> <ol> <li> <p>Integer to Pointer casts</p> <p>Casting an integer to a pointer, is classified as “implementation-defined” in the ANSI C standard (see <a href="https://wiki.sei.cmu.edu/confluence/display/c/INT36-C.+Converting+a+pointer+to+integer+or+integer+to+pointer">here</a>). However, identifying the program points which perform this operation is trivial. Therefore, the pointer analysis implementation, on encountering an integer to pointer cast can easily fall back to saying, “this-pointer-can-point-to-ALL-objects-in-the-program”, thus maintaining soundness, but losing all precision for that particular pointer. The SVF implementation does exactly this by modeling all pointers which are created from integers, to point to a “black hole object”.</p> </li> <li> <p>Buffer overflows</p> <p>Detecting buffer overflows however is a different challenge. Usually, buffers are traversed via a loop that looks something like this (even if it is using Libc functions such as memcpy etc, within memcpy there is a loop that is logically equivalent to this):</p> <p><code class="language-plaintext highlighter-rouge">for (int i = 0; i &lt; SIZE; i++) { ptr++ = buff++; }</code></p> <p>Now, determining the bounds for this loop might be simple, but in general, SIZE might have been the result of a pointer dereference, or even been read from over the network! This type of bounds analysis on loop counters is very challenging, if not impossible to perform at analysis time. Also, note that even if it could be determined at analysis time whether the pointer would go out-of-bounds, it could almost <strong>never</strong> be determined which object would reside at the out-of-bounds location, as <strong>this information is dependent on the heap allocation algorithm being used, order of allocation</strong>, etc, etc.</p> <p>So most pointer analysis implementations that I have seen, assume that a pointer which points to within an object, at the start of a loop, remains within the body of the pointer during all iterations of that loop.</p> <p>From a security perspective, this is actually useful when detecting malicious buffer overflows. We can determine at analysis time which object a particular pointer can point to and for each iteration of the loop, we can instrument the program to insert a check that ensures that the pointer still points to the right object.</p> <p>The only situation where I can see pointer analysis *not* being sound, is if there is intentional buffer overflows – that is the same pointer is incremented (or decremented) in such a way that it accesses adjacent objects that have no relation to each other (not part of a C array, for example).</p> </li> </ol> <p>I’m sure there are other undefined behavior of pointers that cause all sorts of intriguing questions about how they can be correctly modeled and analyzed! Super exciting stuff! :D</p>]]></content><author><name></name></author><category term="pointer-analysis"/><category term="undefined-behavior"/><summary type="html"><![CDATA[Recently, I came across the question — can Pointer Analysis algorithms such as Andersen’s algorithm and Steensgaard’s algorithm, could correctly detect undefined behavior (such as buffer overflow) in C programs? If yes, how? And if no, why not? And if it can’t how does it affect soundness of pointer analysis? This is a very interesting question and I’ll try to share my thoughts in this post.]]></summary></entry><entry><title type="html">Andersen Wave Difference Algorithm</title><link href="https://taptipalit.github.io/blog/2020/andersen-wave-diff/" rel="alternate" type="text/html" title="Andersen Wave Difference Algorithm"/><published>2020-09-22T00:00:00+00:00</published><updated>2020-09-22T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/andersen-wave-diff</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/andersen-wave-diff/"><![CDATA[<p>This is the variant of Andersen’s algorithm that seems to be the “default” in SVF. If you use -ander, this is the analysis that is run. I’ll describe this algorithm in short.</p> <p>The key idea that differentiates this from vanilla-Andersen’s analysis is that instead of propagating the full points-to set, for each CopyEdge, at each iteration, it propagates only the difference between it’s points-to set at the end of the previous iteration, and, its points-to set at this iteration.</p> <p>In order to do this, the constraint graph must not have any cycles, and must be sorted in topological order.</p> <p>At a high level, each iteration is divided into three phases: </p> <p><strong>Phase 1:</strong> First phase of each iteration is to collapse all cycles, and sort the graph in topological order. <strong>Phase 2:</strong> Then, process the Copy constraints/edges and propagate the difference in the points-to sets up the constraint-tree. </p> <p><strong>Phase 3:</strong> Then, process the Load and Store constraints/edges and add the new Copy Edges.</p> <p>If a new Copy Edge is added during Phase 3, then repeat.</p> <p>The worst-case complexity of this approach is the same as vanilla-Andersen, but because it propagates only the difference in points-to sets, it’s faster in the average-case. And also, the Phase 2 and Phase 3 can be parallelized ( according to the paper <a href="http://compilers.cs.ucla.edu/fernando/publications/papers/CGO09.pdf">http://compilers.cs.ucla.edu/fernando/publications/papers/CGO09.pdf</a>)</p> <p>Note: Because Phase 2 and 3 requires an acyclic graph, any positive-weight-cycles are collapsed and the struct object is made field-insensitive.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This is the variant of Andersen’s algorithm that seems to be the “default” in SVF. If you use -ander, this is the analysis that is run. I’ll describe this algorithm in short.]]></summary></entry><entry><title type="html">Cycles in Field Sensitive Pointer Analysis</title><link href="https://taptipalit.github.io/blog/2020/cycles-in-field-sensitive-pointer-analysis/" rel="alternate" type="text/html" title="Cycles in Field Sensitive Pointer Analysis"/><published>2020-09-22T00:00:00+00:00</published><updated>2020-09-22T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/cycles-in-field-sensitive-pointer-analysis</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/cycles-in-field-sensitive-pointer-analysis/"><![CDATA[<p>In the original Field-Sensitive paper by Pearce, this problem was described as the Positive Weight Cycle problem. Let’s first look at this problem as formulated in the original paper.</p> <p>Consider the following C code, that has a void* pointer.</p> <p>typedef struct { int *f1; int *f2; } aggr;</p> <p>int main(void) {</p> <p>    aggr a, *p; void *q;</p> <p>    q = &amp;a; <strong>// q</strong> ⊇ <strong>{a}</strong></p> <p>    p = q; <strong>// p ⊇</strong> <strong>q</strong></p> <p>    q = &amp;(p-&gt;f2); <strong>// q ⊇</strong> <strong>p + 1</strong></p> <p>    // do stuff with q</p> <p>    return 0;</p> <p>}</p> <p>This leads to a weighted cycle – p → q, q – 1 → p. Unlike non-weighted cycles, the nodes in a weighted cycle don’t share the same solution, and it can lead to infinite derivations. </p> <p>For example, </p> <p>q = {a}</p> <p>p = {a}</p> <p>q = {a, r}, where r is the field at the index 1 from the base object a.</p> <p>Then, because it’s a cycle, the derivation continues,</p> <p>p = {a, r}<br/> q = {a, r, s}, where s is the field at the index 1 from the base object r.</p> <p>So on and so forth. Pearce et.al. basically puts a limit to these derivations, and say that the maximum number of fields we’ll support is N.</p> <p>In Andersen’s WaveDiff algorithm, cycles cannot exist because there <em>must</em> be a topological order of all nodes in the constraint graph. Thus, when using Andersen’s WaveDiff algorithm, all nodes in such a <em>Postive-Weighted Cycle</em> are collapsed and <em>all</em> objects they point to are turned field-insensitive.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In the original Field-Sensitive paper by Pearce, this problem was described as the Positive Weight Cycle problem. Let’s first look at this problem as formulated in the original paper. Consider the following C code, that has a void* pointer.]]></summary></entry><entry><title type="html">SVF Details: Variant GEP Constraint Edge</title><link href="https://taptipalit.github.io/blog/2020/variant-gep/" rel="alternate" type="text/html" title="SVF Details: Variant GEP Constraint Edge"/><published>2020-09-22T00:00:00+00:00</published><updated>2020-09-22T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/variant-gep</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/variant-gep/"><![CDATA[<p>GEP Edges can be of two types – Normal Gep Edges and Variant Gep Edges.</p> <p>A normal gep edge is one where the index or offset is known. A variant GEP is a gep whose index is not a constant. For example – </p> <p>struct Obj { int a; int b; int c; }</p> <p>int* ptr = &amp;sobj;</p> <p>for (int i = 0; i &lt; 3; i++) {</p> <p>int *c = (ptr + i); // ← non-constant offset</p> <p>}</p> <p>In this case, a VarGepPE PAGEdge will be inserted for the source ValPN for ptr, and this VarGepPE is converted into a VariantGepCGEdge in the ConstraintGraph and solved in the following way –</p> <p>Src → Dst</p> <p>Any object that the src can point to is first made field-insensitive. Then, these field-insensitive objects are added to the pts-to-set(Dst). </p> <h2 id="variantgeps-and-arrays-of-struct"><strong>VariantGeps and Arrays of struct</strong></h2> <p>The default MemoryModel is field-insensitive when it comes to arrays. All elements in an array are considered to be the same. Now if there is a variable based access to this array, it’ll result in a VarGepPE from this array. </p> <p>Consider this example –</p> <p>struct Obj { int* a; int* b;};</p> <p>struct Obj objects[2];</p> <p>struct Obj* optr = objects;</p> <p>for (int i = 0; i &lt; 2; i++) { <strong>optr[i].b</strong> = &amp;bint; }</p> <p>In this case, the variable i is being used to index into the array objects, via the pointer optr. The IR for the highlighted part will be as follows –</p> <p>for.body:                                         ; preds = %for.cond</p> <p>  %1 = load %struct.Obj*, %struct.Obj** %optr, align 8</p> <p>  %2 = load i32, i32* %i, align 4</p> <p>  %idxprom = sext i32 %2 to i64</p> <p>  <strong>%arrayidx = getelementptr inbounds %struct.Obj, %struct.Obj* %1, i64 %idxprom</strong></p> <p>  <strong>%b = getelementptr inbounds %struct.Obj, %struct.Obj* %arrayidx, i32 0, i32 1</strong></p> <p>  store i32* %bint, i32** %b, align 8</p> <p>  br label %for.inc</p> <p>There are two gep pointers involved in computing the address of optr[i].b. The first one computes the address of the i-th object in the array, and the second one computes the address of the field ‘b’, within the struct. </p> <p>The Constraint / PAG graph considering only the first gep is shown in Figure 1.</p> <p>We’d expect the GEP edge for the second gep instruction to be a normal gep edge because the offset is constant (1), but because the source of this gep is originated from a VarGep, the second GEP edge also becomes a VarGep. Logically it makes sense. The array itself is element/field-insensitive, so it’s impossible to distinguish between fields within the elements of this array.</p> <p>This causes much imprecision in apr-hook framework for httpd. Figure 2 shows the Constraint Graph after the second gep edge is added.</p> <p><strong>Figure 1</strong></p> <p><strong><img src="https://lh7-us.googleusercontent.com/WdtYIttfz1y0BA9g-OZMQQkUu_An-cbNi_1O9MqujUEdnv7I2XWVHeEqk0S2W8kOr6b167FmxVp3ntLn9XyUVKLsCWSeDPTpDa9is8rrYkQe-GcJr4NZwDo9_gXbfWgVQIpOFOfy_jJkH2rcuPIkJg" alt=""/></strong></p> <p><strong>Figure 2</strong></p> <p><img src="https://lh7-us.googleusercontent.com/jPHNZFELrDdRgmThPzgo5BdrnMtpdcDnwZkK4BbCK47zt0lS4c8-UzYydRcupPgV_4lIZLL2nMxmDOUstbG_YuPJIakEe8-Cb5gllAvbAEz2oeIqID8s5GRwg2jDILbkIq9WZfvmLNZmskBG3p9CQA" alt=""/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[GEP Edges can be of two types – Normal Gep Edges and Variant Gep Edges.]]></summary></entry><entry><title type="html">Cycles in the Constraint Graph</title><link href="https://taptipalit.github.io/blog/2020/cycles-in-the-constraint-graph/" rel="alternate" type="text/html" title="Cycles in the Constraint Graph"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/cycles-in-the-constraint-graph</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/cycles-in-the-constraint-graph/"><![CDATA[<p>One of the common optimizations in Andersen’s points-to analysis is cycle elimination. At a source code level, consider the following pointer assignments –</p> <p>int a = 100;<br/> int *p, *q, *r;<br/> p = &amp;a;<br/> q = p;<br/> r = q;<br/> p = r;</p> <p>This represents a cycle, from p → q → r → p. This is a very simple example, and these cycles can happen across functions, and complex situations, so it’s useful to be able to eliminate these cycles.</p> <p>The key point about cycles is that all nodes in a cycle share the same points-to set. So, the analysis can be optimized by replacing all nodes in a cycle by a single node. </p> <p>Now let’s look at SVF’s implementation.</p> <p>Note that only copy constraints can cause cycles. In SVF world, the copy constraints are represented by CopyEdges in the constraint graph. Remember, that during solving the LoadEdges and the StoreEdges, CopyEdges are introduced. These CopyEdges are introduced to make it easier to track cycles. So, anytime a cycle is introduced during solving, it shows up as a cycle of CopyEdges, and all nodes that are part of this cycle can be merged/collapsed into a single Node.</p> <p>Now, let’s take a look at a simple example of a cycle – int *p, *q; p = q; q = p;</p> <p>This is the same example that we were looking at earlier. Figure 3 shows how the cycle of Copy Edges is created in this case. The cycle is nodes 26 → 12 → 22 → 14 → 24 -&gt; 16 -&gt;26. So all of these nodes will be collapsed into a single node. This creates the final constraint graph that looks like Figure 4, where all of these nodes are collapsed into node 26.</p> <p><strong>Figure 3</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/cycle-1.png?w=1024" alt=""/></p> <p><strong>Figure 4</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/cycle-2.png?w=1024" alt=""/></p> <p>Note that each Copy Constraints in the C code, is translated into two constraints in the IR – a Deref/Load constraint, and a Assign/Store constraint, and the effect of solving the two Copy Constraints in the C source code is the same as the effect of solving the two Load and two Store constraints in the IR.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[One of the common optimizations in Andersen’s points-to analysis is cycle elimination. At a source code level, consider the following pointer assignments – int a=100; int *p, *q, *r; p=&amp;a; q=p; r=q; p=r;]]></summary></entry><entry><title type="html">Equivalence of analysis in C Source Code and IR</title><link href="https://taptipalit.github.io/blog/2020/equivalence-of-analysis-in-c-source-code-and-ir/" rel="alternate" type="text/html" title="Equivalence of analysis in C Source Code and IR"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/equivalence-of-analysis-in-c-source-code-and-ir</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/equivalence-of-analysis-in-c-source-code-and-ir/"><![CDATA[<p>The LLVM IR is a representation of the C Source Code. But the constraints generated for the statement in C and in IR might be different. </p> <p>For example, p = *q; is a single constraint of Deref/Load type in C, but results in two Load constraints and one Store constraint in IR. </p> <p>But this doesn’t mean that the result of performing the points-to analysis on C source code is different from performing it on the IR!!! In IR, you just have a more finer view into the memory operations, but eventually, the points-to sets for a memory location / variable will be the same whether you do it in C or LLVM IR.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The LLVM IR is a representation of the C Source Code. But the constraints generated for the statement in C and in IR might be different. ]]></summary></entry><entry><title type="html">Field Sensitive Pointer Analysis</title><link href="https://taptipalit.github.io/blog/2020/field-sensitive-pointer-analysis/" rel="alternate" type="text/html" title="Field Sensitive Pointer Analysis"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/field-sensitive-pointer-analysis</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/field-sensitive-pointer-analysis/"><![CDATA[<p>Field-sensitivity is the ability to distinguish between individual fields of a structure. Field-sensitivity introduces a problem with cycle-elimination, called Positive-Weighted-Cycle which I’ll talk about later. </p> <p>Field Sensitive Analysis was introduced by Pearce et.al. in Efficient Field Sensitive Pointer Analysis. </p> <p>Cycle detection and collapsing is a very important optimization that helps Andersen’s analysis. To recap, a cycle is of the form p⊇ q, ⊇ q r, r ⊇ p. In SVF’s world, it is a CopyEdge from q → p, r → q, and finally from p → r. And all the pointers in this cycle share the same solution, and therefore, can be collapsed.</p> <p>In order to make Andersen’s analysis field-sensitive, Pearce et.al extended the original constraint model with the concept of “weighted constraints” or “weighted constraint edges”. Every field in a complex struct type has an index with respect to the base of the object. And this index is assigned as the weight of the constraint. </p> <p>The constraint itself is of the form p ⊇ q + k, where k is the weight / field-index. </p> <p>In order to derive the solution of this constraint, let’s first recap the solution without the field-sensitivity. The derivation rule looks like –</p> <p>p ⊇ q, q ⊇ {r} then, p ⊇ {r}</p> <p>Now, with the addition of the field constraint, we need to match the field index, in addition with the base-object (r, in this case). So we get –</p> <p>p ⊇ q+k, p {r}, idx(s) = idx(r) + k, idx(s) &lt;= end(r), then, p ⊇ {s}</p> <p>Which is just a mathematical way of saying s is the object at an offset of k from r. Note that s is bounded by end(r), which is the maximum number of fields in the object.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Field-sensitivity is the ability to distinguish between individual fields of a structure. Field-sensitivity introduces a problem with cycle-elimination, called Positive-Weighted-Cycle which I’ll talk about later. ]]></summary></entry><entry><title type="html">Points-to Analysis: Intro</title><link href="https://taptipalit.github.io/blog/2020/points-to-analysis-intro/" rel="alternate" type="text/html" title="Points-to Analysis: Intro"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/points-to-analysis-intro</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/points-to-analysis-intro/"><![CDATA[<p>Points-to Analysis is widely used, but it’s tricky to understand. There’s a wide variety of notations used, leading to confusion. And if that wasn’t bad enough, the most popular points-to analysis tool SVF, operates on LLVM IR, while most descriptions of points-to analysis algorithms use examples from C code. This makes it difficult for readers to map the familiar C source code based examples to the functioning of SVF. </p> <p>This, and the following blog articles will describe things that I have learnt during my experiences with SVF and pointer analysis.</p> <h1 id="andersens-algorithm"><strong>Andersen’s Algorithm</strong></h1> <p>It is an iterative, subset-based points-to analysis algorithm. I’ll describe the different ways to think about the constraints and derivation rules, that I’ve found to be helpful.</p> <ol> <li> <p>p = &amp;q:<br/> This is the Address-of Constraint.<br/> Here the address of q is stored to p. So q ∈ p. Where p is the pts-to set for p.</p> </li> <li> <p>p = q:<br/> This is the Copy Constraint.<br/> Here both p and q are pointers (or pointers to pointers, or pointers to pointers to pointers … and so on). This is fairly intuitive. It means, the points to set of q is a subset of the points-to set of p.<br/> In other words, p might* point to whatever q might point to, in addition to whatever else, p might already be pointing to.<br/> In other words, because q is a subset, we can denote p ⊇ q, denoting p is a superset of q. Some texts use this notation.<br/> In other words, p ⊇ q, lx ∈ q, then,  lx ∈ p.<br/> In other words, the points-to set of q, is copied into the points-to set of p.<br/> Note*: I use might because points-to analysis is imprecise and overapproximated.<br/> Note: the alphabet is used to denote both the pointer / object as well as its points-to set. The meaning is clear depending on the context.</p> </li> <li> <p>*p = q:<br/> This is the Assign Constraint.<br/> Here q is a pointer (or a pointer to a pointer or so on ..) and p is a pointer-to-a-pointer (or a pointer to a pointer to a pointer and so on …). Intuitively, it means that  the points-to set of q is a subset of each of the points-to sets of every element that is in points-to set for p.<br/> In other words, every element that p might point to, might also point to whatever q might point to, in addition to whatever they might already be pointing to.<br/> In other words, for every element e such that, p → e, e q.<br/> In other words, *p ⊇ q, lr p, lx ∈ q , then,  lx ∈ r.<br/> In other words, the points-to set of q, is copied into every element in the points-to set of p.</p> </li> <li> <p>p = *q:<br/> This the Deref Constraint.<br/> Here q is pointer-to-a-pointer (or a pointer-to-a-pointer-to-a-pointer and so on), and p is a pointer (or a pointer-to-a-pointer and so on). Intuitively it means that the points-to set of every element that q could point to, is a subset of the points-to set of p.<br/> In other words, p might point to whatever, every element that is in the points-to set for every element that might be pointed to by q.<br/> In other words, for every element e such that, q → e, p ⊇ e.<br/> In other words, p ⊇ *q, lr ∈ q, lx ∈ r , then,  lx ∈ p.<br/> In other words, the points-to set of every element that q might point to, is copied into the points-to set of p.</p> </li> </ol> <p>The Andersen’s Algorithm, and practically all static analysis algorithms are language-independent. For example, all languages that supports pointers and allows for pointers to be assigned and dereferenced, will work with Andersen’s Analysis.</p> <p>Also, note that C programs can contain more complex statements like **p = ***q, or whatever. These have to be broken down into simpler statements via temporary variables in order to do the analysis.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Points-to Analysis is widely used, but it’s tricky to understand. There’s a wide variety of notations used, leading to confusion. And if that wasn’t bad enough, the most popular points-to analysis tool SVF, operates on LLVM IR, while most descriptions of points-to analysis algorithms use examples from C code. This makes it difficult for readers to map the familiar C source code based examples to the functioning of SVF. ]]></summary></entry><entry><title type="html">SVF Implementation of Andersen’s Analysis</title><link href="https://taptipalit.github.io/blog/2020/svf-implementation-of-andersens-analysis/" rel="alternate" type="text/html" title="SVF Implementation of Andersen’s Analysis"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/svf-implementation-of-andersens-analysis</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/svf-implementation-of-andersens-analysis/"><![CDATA[<p>This brings us to the SVF implementation of Andersen’s Analysis. SVF operates on the LLVM IR. And this presents some quirks. </p> <p>SVF represents the constraints as a constraint graph. As expected there are 4 types of constraints (Address-Of, Copy, Assign, and Deref). Only SVF calls them – </p> <ol> <li> <p>Address-of</p> </li> <li> <p>Copy</p> </li> <li> <p>Store</p> </li> <li> <p>Load</p> </li> </ol> <p>(In addition to these 4, SVF also has a fifth type of constraint to handle field sensitivity, that I’ll talk about later).</p> <p>In the constraint graph, the memory objects and pointers are represented as nodes, and the constraints between them as edges. IR pointers are represented as ValPN nodes, memory objects are represented as ObjPN nodes. Every node, both ValPN and ObjPN has points-to set associated with it.</p> <p>SVF solves constraints by first reducing all Deref (Load) and Assign (Store) constraints to Copy Constraints. Check the underlined part in the Andersen’s Analysis for why this is reasonable. Reducing to Copy Constraints allows SVF to apply optimizations (such as cycle detection, that I’ll talk about later)</p> <p>Let’s take a look at how different LLVM IR instructions result in the creation of new nodes and edges in the constraint graph (Actually in the PAG graph, but let’s defer the discussion of PAG vs Constraint graph for later). </p> <ol> <li> <p>AllocaInst: This instruction is used to create a local variable on the stack. When SVF sees this instruction, say p = alloca i32**, corresponding to the C statement int *p, it creates two nodes. One for the pointer p (a ValPN node), and one for the memory object (a ObjPN node). The memory object represents the memory associated with the variable.</p> <p>An Addr Edge is added between the ObjPN and the ValPN, denoting the address-of relationship. When this AddrEdge constraint is solved, points-to set of the ValPN includes the ObjPN.</p> <p>This is exactly the same thing as solving the Addr-of constraint. p = &amp;q.</p> <p>Though the language changes ( C → IR) the rules that guide the derivation of the constraints don’t change. A Copy Constraint derives in the same way in the IR, as in C.</p> <p>For example, consider the bolded C statements from the simple C program, whose Constraint Graph is shown in the figure below –</p> <p><strong>int a = 100;<br/> int *p, *q;</strong><br/> p = &amp;a;<br/> q = p;<br/> p = q;</p> <p>Translated to IR – %a = alloca i32, align 4<br/> %p = alloca i32*, align 8<br/> %q = alloca i32*, align 8</p> </li> </ol> <p>This creates three ValPNs and their corresponding three ObjPNs. The Green edges are the Address Edges.</p> <p>Each Node in the ConstraintGraph has a unique id (19, 20, etc).</p> <p><strong>Figure 1:</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/conscg_initial.png?w=1024" alt=""/></p> <ol> <li> <p>LoadInst: This instruction is used to explicitly load the contents of an IR pointer into a virtual register. When SVF sees this instruction, say %0 = load i32*, i32** q, it creates one ValPN node for the virtual register %0, and adds a LoadEdge between the ValPN node for q (not the ObjPN) and the ValPN node for %0.</p> <p>Consider the bolded statement in the C program, </p> </li> </ol> <p>int a = 100;<br/> int *p, *q;<br/> p = &amp;a;<br/> <strong>q = p;</strong><br/> p = q;</p> <p>This is translated to the IR instructions:</p> <p><strong>%0 = load i32*, i32** %p, align 8</strong></p> <p>  <strong>store i32* %0, i32** %q, align 8</strong>  %1 = load i32*, i32** %q, align 8</p> <p>  store i32* %1, i32** %p, align 8</p> <p>Let’s look at the first Load instruction. In the above constraint graph it is represented by the Red edge from nodes 11 → 20.</p> <p>This LoadEdge represents a Deref constraint. As stated before, SVF reduces this to a Copy Constraint from all nodes that exist in the points-to set (source) to the destination of the load. So in the simplest case, LoadEdge will lead to the creation of a CopyEdge from the ObjPN that existed in the source ValPN’s pts-to set (added because of the AddrEdge), to the ValPN of the destination.</p> <p>[Quick tip: Think about how you’d solve p = *q. p is the destination, q is the source of this load. You’d first find all elements you know q to point to, (call them e1, e2<em>,</em> etc) and then add copy constraints q = e1, q = e2, etc. The source is the e’s and the destination is q.]</p> <p>Solving the two LoadEdges (11 → 20 and 13 → 22) results in adding the following CopyEdges (black edges). Note, that the points-to sets of the relevant pointers are denoted as {node-id} beside the pointers.<br/> <strong>Figure 2</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/andersen-1.png?w=1024" alt=""/></p> <ol> <li> <p>StoreInst: This instruction is used to explicitly store the contents of a virtual register to a memory location. When SVF sees this instruction, say  store i32* %1, i32** %p, it creates an StoreEdge from the source ValPN (%1) to the destination ValPN (%p).</p> <p>This StoreEdge represents an Assign Constraint. In the simplest case, this will be reduced to a CopyEdge from the ValPN of the source pointer, to the ObjPN in the pts-to set of the ValPN of the destination pointer (added because of the AddrEdge)</p> <p>Let’s continue with the previous IR instructions –</p> <p>%0 = load i32*, i32** %p, align 8</p> </li> </ol> <p> <strong>store i32* %0, i32** %q, align 8</strong><br/> %1 = load i32*, i32** %q, align 8</p> <p> <strong>store i32* %1, i32** %p, align 8</strong><br/> [Quick Tip: In order to understand how SVF solves this, think about the assign constraint *p = q. To solve this, you’d first find all elements e that exist in the points to set for p, and then add a Copy Constraint e = q.]</p> <p>The three Store Edges add the three (bolded, dashed) black Copy Edges.</p> <p><strong>Figure 3</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/andersen-2-1.png?w=1024" alt=""/></p> <p>CastInst: CastInst instructions do BitCasts and other casts and these result in CopyEdges in the LLVM IR. CopyEdges are solved in the same way as Copy Constraints in the Andersen’s analysis.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This brings us to the SVF implementation of Andersen’s Analysis. SVF operates on the LLVM IR. And this presents some quirks. ]]></summary></entry><entry><title type="html">SVF’s Field-Sensitivity: Handling of GEP Edges</title><link href="https://taptipalit.github.io/blog/2020/svfs-field-sensitivity-handling-of-gep-edges/" rel="alternate" type="text/html" title="SVF’s Field-Sensitivity: Handling of GEP Edges"/><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T00:00:00+00:00</updated><id>https://taptipalit.github.io/blog/2020/svfs-field-sensitivity-handling-of-gep-edges</id><content type="html" xml:base="https://taptipalit.github.io/blog/2020/svfs-field-sensitivity-handling-of-gep-edges/"><![CDATA[<p>Let’s now look at how SVF handles the simpler cases of field-sensitive analysis (without any cycle elimination and positive-weighted-cycles)</p> <p>Imagine a struct A {int idx; struct A* next};<br/> And consider the C statement:<br/> p-&gt;idx = 10; // p is a pointer pointing to an object of type A</p> <p>Clearly, this statement involves computing the address of the field ‘idx’ of the type struct A. In LLVM IR, this is done by the Instruction GetElementPtrInst. It looks like –</p> <p>%idx = getelementptr inbounds %struct.A, %struct.A* %1, i32 0, i32 0</p> <p>idx is a virtual register that contains the address of the field ‘idx’.</p> <p>The GEP instruction has its own dedicated page on LLVM website :) [https://llvm.org/docs/GetElementPtr.html</p> <p>](https://llvm.org/docs/GetElementPtr.html)Anyhow, what we need to remember is that GEP computes a pointer into a struct type. It doesn’t access any memory, it’s purely pointer computation. And the last index (0 in this case) denotes the index of the field being accessed (0 means the first field – that is, ‘idx’)</p> <p>Now, every time SVF encounters a GEP instruction, it creates a GEPEdge from the base pointer to the field pointer. The base pointer is the IR pointer to the object, and field pointer is the IR pointer to the field within the object. This means that there can be multiple GEPEdges for the same field, if the same field is accessed multiple times. It simply means that the address of that field was computed multiple times (and loaded into a virtual register).</p> <p>So, let’s take a look at what that looks like in the SVF Constraint Graph. Consider the C source code –</p> <p>struct A {int idx; int *p};</p> <p>int main( void){</p> <p>    int k = 10;</p> <p>    struct A aobj;</p> <p>    aobj.p = &amp;k;</p> <p>    return 0;</p> <p>}</p> <p>The IR instructions – </p> <p>  %k = alloca i32, align 4</p> <p>  %aobj = alloca %struct.A, align 8</p> <p>  store i32 0, i32* %retval, align 4</p> <p>  store i32 10, i32* %k, align 4</p> <p>  <strong>%p = getelementptr inbounds %struct.A, %struct.A* %aobj, i32 0, i32 1</strong></p> <p>  store i32* %k, i32** %p, align 8</p> <p>The initial constraint graph is shown in Figure 5. The purple edge is the GepEdge (There are actually two types of Gep Edges, but for now, let’s talk only about the “Normal” GepEdge – the NormalGepCGEdge). It does not show in the figure, but every Normal Gep Edge has the field index associated with it. And there can be multiple edges with the same index, if the field at the same index is accessed multiple times.</p> <p><strong>Figure 5.</strong> </p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/field-cycle-1.png?w=1024" alt=""/></p> <p>Conceptually, a GepEdge is similar to a CopyEdge, with an added constraint on which fields are copied (similar to how Pearce et.al extended Andersen’s plain copy constraint). Figure 5 shows an example. Let’s focus only on nodes 12, 11, and 17. First the AddressEdge will make sure that pts-to-set(11) = {12}. Then, the purple GepEdge will act like a Copy Constraint, but instead of copying the whole pts-to-set(11), it’ll copy only the objects corresponding to the field of the GepEdge. In this case, the GepEdge corresponded to field index 1 (the second field), so it’ll copy only the second field of the ObjPN with id 12.</p> <p>Now, as you can see, there’s no such node in the graph (yet). So far, we just have a single node for the ObjPN for aobj. So SVF will create this ObjPN – a GepObjPN, with base node-id as 12, and field-index as 1. Assume this GepObjPN gets a NodeId of 20, then points-to-set(17) = {20}.</p> <p>If the second field is accessed again, a different GepEdge will be created, but solving it will result in the same GepObjPN node with NodeId = 20, being copied into the points-to-set of the destination of the GepEdge. Figure 6, shows the constraint graph at this stage.</p> <p><strong>Figure 6</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/field-cycle-2.png?w=1024" alt=""/></p> <p>Note: The handling for the first field (0th index) is slightly different because the pointer to the struct and the pointer to the first field can be used interchangeably. I’ll talk about it later (if I get around to it).</p> <p>Note: The ObjPN corresponding to the whole object aobj is a FIObjPN (Field Insensitive Obj PN), but this is a bit of a misnomer. It does not mean that the object itself is field-insensitive. It just means that you can’t use this FIObjPN to talk about the individual fields of the object.</p> <p>Now, let’s take a look at how the StoreEdge from Node 9 to Node 17 will be processed. As discussed earlier, the StoreEdge will be reduced to a CopyEdge from the source of the StoreEdge, to all nodes that exist in the points-to set of the destination. Thus, it adds a CopyEdge from 9 to 20. This is the final Constraint Graph, shown in Figure 7.</p> <p><strong>Figure 7</strong></p> <p><img src="https://tpalitblog.files.wordpress.com/2023/11/conscg_final.png?w=1024" alt=""/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let’s now look at how SVF handles the simpler cases of field-sensitive analysis (without any cycle elimination and positive-weighted-cycles) Imagine a struct A {int idx; struct A* next}; And consider the C statement: p-&gt;idx = 10; // p is a pointer pointing to an object of type A Clearly, this statement involves computing the address of the field ‘idx’ of the type struct A. In LLVM IR, this is done by the Instruction GetElementPtrInst. It looks like – %idx = getelementptr inbounds %struct.A, %struct.A* %1, i32 0, i32 0]]></summary></entry></feed>